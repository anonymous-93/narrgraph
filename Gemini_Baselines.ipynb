{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Base Code"
      ],
      "metadata": {
        "id": "1o1kZzBIEljg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_jTukhOEeo2"
      },
      "outputs": [],
      "source": [
        "#Path for the lusa news dataset\n",
        "path_lusa=\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set_ids = ['lusa_97',\n",
        " 'lusa_4',\n",
        " 'lusa_67',\n",
        " 'lusa_20',\n",
        " 'lusa_83',\n",
        " 'lusa_104',\n",
        " 'lusa_80',\n",
        " 'lusa_79',\n",
        " 'lusa_34',\n",
        " 'lusa_47',\n",
        " 'lusa_30',\n",
        " 'lusa_96',\n",
        " 'lusa_11',\n",
        " 'lusa_112',\n",
        " 'lusa_100',\n",
        " 'lusa_77',\n",
        " 'lusa_38',\n",
        " 'lusa_86',\n",
        " 'lusa_60']"
      ],
      "metadata": {
        "id": "5ZR27_0HE-VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "98KgunuSFTc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files=list()\n",
        "for i in os.listdir(path_lusa):\n",
        "  if(i.startswith(\"lusa\")):\n",
        "    files.append(i.split(\".\")[0])\n"
      ],
      "metadata": {
        "id": "5CfTmr6AFcKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files=set(files)"
      ],
      "metadata": {
        "id": "tpSxx0DaFxmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_ids=[f for f in files if f not in test_set_ids]"
      ],
      "metadata": {
        "id": "hMrGSDrqF1pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_ids"
      ],
      "metadata": {
        "id": "0XmA3sIWGUmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The annotation types used in the paper\n",
        "ann_types=[\"Event\",\n",
        "           \"Participant\",\n",
        "           \"Time\",\n",
        "           \"Spatial_Relation\",\n",
        "           \"TLINK\",\n",
        "           \"SRLINK\",\n",
        "           \"QSLINK\",\n",
        "           \"OLINK\"\n",
        "           ]"
      ],
      "metadata": {
        "id": "E9u18E4VIVoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##See the documents with Counts for the Different Annotations"
      ],
      "metadata": {
        "id": "t83wOO3zGeLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used to determine what document would be used for the one shot generative approach"
      ],
      "metadata": {
        "id": "1cbHDs4bPMlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Function to compile the number of annotation of a given file\n",
        "from collections import Counter\n",
        "def getAnnCount(file=\"lusa_10.ann\"):\n",
        "  anns=list()\n",
        "  with open(os.path.join(path_lusa,file),\"r\") as f:\n",
        "     for line in f:\n",
        "        a=line.split(\"\\t\")[1].split(\" \")[0]\n",
        "        for an_t in ann_types:\n",
        "          if a.startswith(an_t):\n",
        "            anns.append(a)\n",
        "  count=dict(Counter(anns))\n",
        "  count[\"id\"]=file\n",
        "  return count"
      ],
      "metadata": {
        "id": "eSF_qRhTGmnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#iterate through all the files and generate a dataset\n",
        "\n",
        "result=list()\n",
        "for doc in train_set_ids:\n",
        "  stat=getAnnCount(doc+\".ann\")\n",
        "  #print(stat[\"id\"])\n",
        "  result.append(stat)\n",
        "\n"
      ],
      "metadata": {
        "id": "1uHRoZIhHLNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df=pd.DataFrame.from_dict(result)"
      ],
      "metadata": {
        "id": "-hvh5hHqL7Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "DSuUjIWkPlyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entity extraction prompt using lang extract"
      ],
      "metadata": {
        "id": "PVSSRM1bTk5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langextract"
      ],
      "metadata": {
        "id": "k_j7qmkyqMND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langextract as lx"
      ],
      "metadata": {
        "id": "oqI4hOqPqP3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_description = f'''Tarefa\n",
        "\n",
        "Analisa o texto abaixo (em Português Europeu) e extrai eventos, participantes, expressões temporais (TIMEX) e relações espaciais, respeitando rigorosamente as instruções seguintes.\n",
        "Não omitas informação relevante nem introduzas elementos que não estejam explicitamente presentes no texto.\n",
        "\n",
        "1. Eventos\n",
        "\n",
        "Identifica todas as situações temporalmente relevantes, incluindo:\n",
        "\n",
        "verbos principais (exclui verbos auxiliares),\n",
        "\n",
        "nominalizações com interpretação de acontecimento ou de estado,\n",
        "\n",
        "adjetivos predicativos que exprimem estados relevantes,\n",
        "\n",
        "expressões (semi)lexicalizadas que funcionem como predicador.\n",
        "\n",
        "Regras de decisão:\n",
        "\n",
        "Verbos aspetuais (ex. começar, continuar, terminar) devem ser identificados como eventos autónomos e associados ao evento principal.\n",
        "\n",
        "Em construções copulativas, marca o adjetivo ou o nome (o predicativo do sujeito) que estiver a seguir ao verbo copulativo.\n",
        "\n",
        "Em construções com ir + infinitivo:\n",
        "\n",
        "se os verbos forem adjacentes, identifica apenas o verbo principal;\n",
        "\n",
        "se não forem adjacentes, identifica ambos como eventos distintos.\n",
        "\n",
        "Em sintagmas preposicionais, identifica apenas o núcleo nominal.\n",
        "\n",
        "Exclui nominalizações que não tenham leitura eventiva.\n",
        "\n",
        "2. Expressões temporais (TIMEX)\n",
        "\n",
        "Identifica todas as expressões que denotem:\n",
        "\n",
        "tempo de calendário,\n",
        "\n",
        "hora do dia,\n",
        "\n",
        "duração,\n",
        "\n",
        "frequência temporal.\n",
        "\n",
        "Inclui apenas:\n",
        "\n",
        "sintagmas nominais temporais (ex. o verão passado, três dias),\n",
        "\n",
        "advérbios temporais (ex. ontem, recentemente).\n",
        "\n",
        "Exclui:\n",
        "\n",
        "preposições e conetores temporais (durante, quando, enquanto),\n",
        "\n",
        "expressões vagas não quantificáveis (frequentemente, raramente),\n",
        "\n",
        "nomes próprios usados como datas (25 de Abril).\n",
        "\n",
        "3. Participantes\n",
        "\n",
        "Identifica todas as entidades relevantes envolvidas nos eventos, incluindo:\n",
        "\n",
        "sintagmas nominais completos (com determinantes, modificadores e complementos),\n",
        "\n",
        "pronomes com referência a entidades do discurso.\n",
        "\n",
        "Cada participante deve corresponder a uma entidade concreta ou abstrata que intervenha, seja afetada ou seja localizada por um evento.\n",
        "\n",
        "4. Relações espaciais\n",
        "\n",
        "Identifica todas as expressões que codificam relações espaciais, quer de localização estática quer de movimento, incluindo:\n",
        "\n",
        "sintagmas preposicionais ou adverbiais (ex. em, para, desde, perto de, ao longo de).\n",
        "\n",
        "Sempre que possível, distingue:\n",
        "\n",
        "relações de localização,\n",
        "\n",
        "relações de direção,\n",
        "\n",
        "relações de origem, percurso ou destino.\n",
        "\n",
        "Cada item deve ter um identificador único.\n",
        "\n",
        "Mantém consistência entre elementos referenciados.\n",
        "\n",
        "Não inventes informação nem faças inferências não suportadas pelo texto.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "2zhptRn2nmWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text from the few shot document chosen (lusa_106)\n",
        "text_few_shot=f'''\n",
        "Porto, 16 dez 2020\n",
        "VSYM (PM) // JAP\n",
        "Grua de camião derrubou poste de iluminação no Porto e matou homem de 52 anos\n",
        "Um homem de 52 anos morreu hoje, e um outro sofreu ferimentos ligeiros, na sequência de um acidente com um camião que derrubou um poste de eletricidade na Rua da Constituição, no Porto, disse à Lusa fonte da PSP.\n",
        "De acordo com a fonte das Relações Públicas do Comando Metropolitano da PSP do Porto, o poste de iluminação foi derrubado pela grua do camião atingindo o homem que circulava na rua.\n",
        "A vítima ainda foi transportada com vida para o Hospital de Santo António, mas acabou por morrer.\n",
        "A estrutura atingiu ainda o funcionário de umas bombas de gasolina, que sofreu ferimentos ligeiros.\n",
        "O acidente ocorreu cerca das 07:50.\n",
        "Em comunicado, a Câmara do Porto, esclareceu, entretanto, que o acidente que provocou a queda de um poste de eletricidade envolveu um veículo da Empresa Municipal de Ambiente do Porto e já decidiu averiguar o que terá acontecido.\n",
        "\"A empresa ordenou já a abertura de um processo de averiguação interna às causas deste acidente estando também, naturalmente, disponível para colaborar com as autoridades\", assinala o município lamentando a morte e endereçando as condolências à sua família.\n",
        "'''"
      ],
      "metadata": {
        "id": "RqlvBaDHoADB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to get the annotations for the few shot example and convert them in the lang extract Extraction object. Returns a list of all annotations\n",
        "def getFewShotExamples(file=\"lusa_106\"):\n",
        "  fs=list()\n",
        "  with open(os.path.join(path_lusa,file+\".ann\"),\"r\") as f:\n",
        "     for line in f.read().splitlines():\n",
        "        atrs=line.split(\"\\t\")\n",
        "        cls=atrs[1].split(\" \")[0]\n",
        "\n",
        "        if cls in ann_types[0:4]:# apenas entidades\n",
        "             #print(atrs)\n",
        "             text=atrs[2]\n",
        "             fs.append(lx.data.Extraction(extraction_class=cls, extraction_text=text),)\n",
        "  return fs"
      ],
      "metadata": {
        "id": "Fkj3D2oeob_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d=getFewShotExamples()\n",
        "print(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63P-dAq-qEhr",
        "outputId": "87d8dac9-c835-42cd-934b-246b82be0655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Extraction(extraction_class='Time', extraction_text='16 dez 2020', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='derrubou', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='matou', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='Um homem de 52 anos', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='morreu', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Time', extraction_text='hoje', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='um outro', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='sofreu', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='ferimentos ligeiros', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='acidente', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='um camião', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='que', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='derrubou', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='um poste de eletricidade', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Spatial_Relation', extraction_text='n', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='a Rua da Constituição', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Spatial_Relation', extraction_text='n', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='o Porto', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='disse', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='à Lusa', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='fonte', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='a PSP', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='acordo', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='a fonte', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='as Relações Públicas', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='o Comando Metropolitano da PSP', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='o Comando Metropolitano da PSP do Porto', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='o Porto', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='o poste de iluminação', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='derrubado', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='a grua', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='o camião', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='atingindo', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='o homem', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='que', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='circulava', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Spatial_Relation', extraction_text='n', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='a rua', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='A vítima', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='transportada', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='com vida', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Spatial_Relation', extraction_text='para', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='o Hospital de Santo António', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='acabou por morrer', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='A estrutura', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='atingiu', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='o funcionário', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='umas bombas de gasolina', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='que', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='sofreu', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='ferimentos ligeiros', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='acidente', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='ocorreu', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Time', extraction_text='07:50', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='comunicado', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='a Câmara do Porto', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='a Câmara', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Spatial_Relation', extraction_text='d', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='o Porto', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='esclareceu', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='acidente', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='que', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='provocou', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='queda', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='um poste de eletricidade', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='envolveu', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='um veículo', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='a Empresa Municipal de Ambiente do Porto', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='a Empresa Municipal de Ambiente', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Spatial_Relation', extraction_text='d', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='o Porto', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='decidiu', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='averiguar', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='terá acontecido', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='A empresa', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='ordenou', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='abertura', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='processo', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='averiguação interna', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='causas', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='acidente', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='estando disponível', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='colaborar', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='as autoridades', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='assinala', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='o município', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='lamentando', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='morte', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Event', extraction_text='endereçando', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='as condolências', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='sua', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None), Extraction(extraction_class='Participant', extraction_text='família', char_interval=None, alignment_status=None, extraction_index=None, group_index=None, description=None, attributes=None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langextract as lx\n",
        "import json\n",
        "\"\"\"\n",
        "HERE SHOULD BE THE KEY\n",
        "\"\"\"\n",
        "LANGEXTRACT_API_KEY=\"\" #place Google AI Studio Gemini Key here\n",
        "\n",
        "\"\"\"\n",
        "HERE SHOULD BE THE KEY\n",
        "\"\"\"\n",
        "#lusa_106 -> used as fewshot\n",
        "\n",
        "def extract_info_gemini(input_text,prompt_description):\n",
        "    example_text=text_few_shot\n",
        "  # Define extraction prompt\n",
        "    prompt_description = prompt_description\n",
        "\n",
        "      # Define example data with entities in order of appearance\n",
        "    examples = [\n",
        "          lx.data.ExampleData(\n",
        "              text=example_text,\n",
        "              extractions=getFewShotExamples()\n",
        "          )\n",
        "    ]\n",
        "    result = lx.extract(\n",
        "        text_or_documents=input_text,\n",
        "        prompt_description=prompt_description,\n",
        "        examples=examples,\n",
        "        api_key=LANGEXTRACT_API_KEY,\n",
        "        model_id=\"gemini-2.5-flash-lite\",\n",
        "    )\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "mdJ01n8YTmeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h26yzfxdzCG2"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6i3R94zfzOe",
        "outputId": "e4ce4f5e-6ef8-494e-8b3b-07d8b1398251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lusa_22\n"
          ]
        }
      ],
      "source": [
        "# test example\n",
        "test_example=train_set_ids[10]\n",
        "print(test_example)\n",
        "with open(os.path.join(path_lusa,test_example+\".txt\"),\"r\") as f:\n",
        "  test_text=f.read()\n",
        "\n",
        "result=extract_info_gemini(test_text,prompt_description)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yeGNJ4w8uXeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODmncrtmcaVW",
        "outputId": "e67b348f-8bdc-4059-86d0-e029db447c69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[94m\u001b[1mLangExtract\u001b[0m: Saving to \u001b[92mextraction_results.jsonl\u001b[0m: 1 docs [00:00, 220.43 docs/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m✓\u001b[0m Saved \u001b[1m1\u001b[0m documents to \u001b[92mextraction_results.jsonl\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\u001b[94m\u001b[1mLangExtract\u001b[0m: Loading \u001b[92mextraction_results.jsonl\u001b[0m: 100%|██████████| 17.4k/17.4k [00:00<00:00, 9.42MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m✓\u001b[0m Loaded \u001b[1m1\u001b[0m documents from \u001b[92mextraction_results.jsonl\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Save the results to a JSONL file\n",
        "lx.io.save_annotated_documents([result], output_name=\"extraction_results.jsonl\", output_dir=\".\")\n",
        "\n",
        "# Generate the visualization from the file\n",
        "html_content = lx.visualize(\"extraction_results.jsonl\")\n",
        "with open(\"visualization.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    if hasattr(html_content, 'data'):\n",
        "        f.write(html_content.data)  # For Jupyter/Colab\n",
        "    else:\n",
        "        f.write(html_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FULL TEST DATA"
      ],
      "metadata": {
        "id": "67wTsbt9wXjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_set_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIefxt1YwcUR",
        "outputId": "f0003b6c-ecf3-4629-8bbf-a875c966350d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lusa_97',\n",
              " 'lusa_4',\n",
              " 'lusa_67',\n",
              " 'lusa_20',\n",
              " 'lusa_83',\n",
              " 'lusa_104',\n",
              " 'lusa_80',\n",
              " 'lusa_79',\n",
              " 'lusa_34',\n",
              " 'lusa_47',\n",
              " 'lusa_30',\n",
              " 'lusa_96',\n",
              " 'lusa_11',\n",
              " 'lusa_112',\n",
              " 'lusa_100',\n",
              " 'lusa_77',\n",
              " 'lusa_38',\n",
              " 'lusa_86',\n",
              " 'lusa_60']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set_ids[13]\n"
      ],
      "metadata": {
        "id": "fo8ukmKh3IL2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1a0f4305-0650-498c-d925-210d23eac950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lusa_112'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_folder=\"results/entities_outputs\" # the output folder where to place the result files\n",
        "import time\n",
        "#time.sleep(60*60)\n",
        "for test_doc in test_set_ids:\n",
        "  #time.sleep(60*30)\n",
        "  print(test_doc)\n",
        "  with open(os.path.join(path_lusa,test_doc+\".txt\"),\"r\") as f:\n",
        "    test_text=f.read()\n",
        "  result=extract_info_gemini(test_text,prompt_description)\n",
        "  lx.io.save_annotated_documents([result], output_name=os.path.join(output_folder,\"test_results\",test_doc+\"_results.jsonl\"), output_dir=\".\")\n",
        "\n",
        "  # Generate the visualization from the file\n",
        "  html_content = lx.visualize(os.path.join(output_folder,\"test_results\",test_doc+\"_results.jsonl\"))\n",
        "  with open(os.path.join(output_folder,\"test_results\",test_doc+\"_visualization.html\"), \"w\", encoding=\"utf-8\") as f:\n",
        "      if hasattr(html_content, 'data'):\n",
        "          f.write(html_content.data)  # For Jupyter/Colab\n",
        "      else:\n",
        "          f.write(html_content)"
      ],
      "metadata": {
        "id": "2lxnaxchwn_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Relation Extraction prompt usinge Google GenerativeAI library"
      ],
      "metadata": {
        "id": "C3d7QQC80Ajv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to get the  Relations from an annotation file to used as few-shot\n",
        "def getFewShotRelationExamples(file=\"lusa_106\",json=False):\n",
        "  fs=list()\n",
        "  with open(os.path.join(path_lusa,file+\".ann\"),\"r\") as f:\n",
        "     for line in f.read().splitlines():\n",
        "        atrs=line.split(\"\\t\")\n",
        "        cls=atrs[1].split(\" \")[0]\n",
        "        cls=cls.split(\"_\")[0] #separate TLINK de TLINK_identity\n",
        "        if cls in ann_types[4:]:# apenas entidades\n",
        "            args=atrs[1].split(\" \")[1:]\n",
        "            fs.append((cls,args[0].split(\":\")[1],args[1].split(\":\")[1]))\n",
        "\n",
        "  #map arguments\n",
        "  map_t=dict()\n",
        "  with open(os.path.join(path_lusa,file+\".ann\"),\"r\") as f:\n",
        "     for line in f.read().splitlines():\n",
        "        entry=line.split(\"\\t\")\n",
        "        id=entry[0]\n",
        "        if id.startswith(\"T\"):\n",
        "          inout=entry[1].split(\" \")\n",
        "          begin=inout[1]\n",
        "          end=inout[2]\n",
        "          #print(entry)\n",
        "          if json:\n",
        "            map_t[id]={'begin': begin, 'end': end, 'span':entry[2]}\n",
        "          else:\n",
        "            map_t[id]=f'begin: {begin}, end: {end}, span:{entry[2]}'\n",
        "\n",
        "\n",
        "  fs_mapped=list()\n",
        "  for cl,arg1,arg2 in fs:\n",
        "    arg1_m=map_t[arg1]\n",
        "    arg2_m=map_t[arg2]\n",
        "    if(json):\n",
        "      fs_mapped.append({\"class\":cl,\"arg1\":arg1_m,\"arg2\":arg2_m})\n",
        "    else:\n",
        "\n",
        "      fs_mapped.append(f'{{class: {cl}, arg1:{{{arg1_m}}}, arg2:{{{arg2_m}}}}}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  if json:\n",
        "    return(fs_mapped)\n",
        "\n",
        "  return \"\\n\".join(fs_mapped)\n"
      ],
      "metadata": {
        "id": "21TKWNQl0cl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test it\n",
        "s=getFewShotRelationExamples(\"lusa_83\",json=True)\n",
        "s"
      ],
      "metadata": {
        "id": "rmoJobjd825H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "path_results_entities=\"\" #folder where the results for the entities were saved (same as output folder above)\n",
        "\n",
        "#function to get the Entities from the previous extraction step for each document\n",
        "def getEntities2Consider(file=\"lusa_100\",d=False):\n",
        "  file_ext=file+\"_results.jsonl\"\n",
        "\n",
        "\n",
        "  with open(os.path.join(path_results_entities,file_ext)) as f:\n",
        "      first_line = f.readline()\n",
        "      result_entity=json.loads(first_line)\n",
        "  entities_2_consider=list()\n",
        "  for entry in result_entity[\"extractions\"]:\n",
        "    temp=dict()\n",
        "    #print(entry)\n",
        "    try:\n",
        "      temp[\"class\"]=entry[\"extraction_class\"]\n",
        "      temp[\"text\"]=entry[\"extraction_text\"]\n",
        "      temp[\"begin\"]=entry[\"char_interval\"][\"start_pos\"]\n",
        "      temp[\"end\"]=entry[\"char_interval\"][\"end_pos\"]\n",
        "      if not d:\n",
        "\n",
        "        str_out=f'classe: {temp[\"class\"]}, texto: {temp[\"text\"]}, begin: {temp[\"begin\"]}, end: {temp[\"end\"]}'\n",
        "        entities_2_consider.append(str_out)\n",
        "      else:\n",
        "        entities_2_consider.append(temp)\n",
        "    except Exception as e:\n",
        "      print(\"Failed to extract entity to consider \", str(e))\n",
        "  return entities_2_consider\n"
      ],
      "metadata": {
        "id": "1FSykVlg9QcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "import json\n",
        "getEntities2Consider(\"lusa_4\",True)"
      ],
      "metadata": {
        "id": "KSn9HT9X9kIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to build the prompt. It receives the few shot text document, the relation annotations from that document and the test document.\n",
        "#The entities for the test document are integrated via the test_doc id passed as an argument\n",
        "\n",
        "def getRelationPrompt(text_few_shot,fs,test_doc):\n",
        "\n",
        "  with open(os.path.join(path_lusa,test_doc+\".txt\"),\"r\") as f:\n",
        "    test_text=f.read()\n",
        "\n",
        "\n",
        "  prompt_relations=f'''\n",
        " Tarefa\n",
        "Recebes:\n",
        "\n",
        "um texto em Português Europeu;\n",
        "as anotações já fornecidas de eventos, participantes, expressões temporais (TIMEX) e relações espaciais, cada uma identificada pelo respetivo span textual.\n",
        "\n",
        "O teu objetivo é extrair ligações explícitas entre essas entidades.\n",
        "\n",
        "Extrai apenas as seguintes classes de links:\n",
        "\n",
        "TLINK — ligação temporal\n",
        "\n",
        "OLINK — ligação referencial/objectal\n",
        "\n",
        "QSLINK — ligação espacial qualitativa\n",
        "\n",
        "SRLINK — ligação semântica entre evento e participante ou entre participante e participante\n",
        "\n",
        "Restrições\n",
        "\n",
        "Usa exclusivamente o texto literal das entidades anotadas como argumentos.\n",
        "\n",
        "Não inferir relações: cria links apenas quando a ligação é claramente expressa no texto.\n",
        "\n",
        "Produz apenas triplos, sem qualquer explicação adicional.\n",
        "\n",
        "1) TLINK (Temporal Link)\n",
        "\n",
        "Cria um TLINK quando o texto estabelece explicitamente uma relação temporal entre:\n",
        "\n",
        "evento <-> evento\n",
        "\n",
        "evento <-> expressão temporal\n",
        "\n",
        "expressão temporal<-> expressão temporal\n",
        "\n",
        "Direção do link\n",
        "\n",
        "evento <-> tempo -> arg1 = evento, arg2 = expressão temporal\n",
        "\n",
        "evento <-> evento ou tempo <-> tempo ->\n",
        "arg1 = elemento mais recente no texto,\n",
        "arg2 = elemento anterior\n",
        "\n",
        "\n",
        "2) OLINK (Objectal Link)\n",
        "\n",
        "Cria um OLINK quando duas menções referem explicitamente:\n",
        "\n",
        "a mesma entidade (correferência),\n",
        "\n",
        "uma relação parte–todo,\n",
        "\n",
        "uma relação membro–conjunto ou subconjunto–conjunto,\n",
        "\n",
        "uma disjunção referencial explícita.\n",
        "\n",
        "Direção do link\n",
        "\n",
        "arg1 = menção dependente (ex. pronome, descrição anafórica)\n",
        "\n",
        "arg2 = menção antecedente\n",
        "\n",
        "\n",
        "\n",
        "3) QSLINK (Qualitative Spatial Link)\n",
        "\n",
        "Cria um QSLINK quando uma relação espacial estática liga explicitamente:\n",
        "\n",
        "uma figura (evento ou participante localizado),\n",
        "\n",
        "a um ground (local).\n",
        "\n",
        "Direção do link\n",
        "\n",
        "arg1 = figura\n",
        "\n",
        "arg2 = ground\n",
        "\n",
        "\n",
        "4) SRLINK (Semantic Role Link)\n",
        "\n",
        "Cria um SRLINK quando um evento tem um participante (ou outro evento) como argumento explícito no texto.\n",
        "\n",
        "Direção do link\n",
        "\n",
        "arg1 = evento\n",
        "\n",
        "arg2 = participante (ou evento subordinado)\n",
        "\n",
        "\n",
        "Formato final de saída\n",
        "\n",
        "\n",
        "Apenas relações no formato (classe: tipo da relação, arg1: {{begin, end, span}}, arg2: {{begin, end, span}}\n",
        "\n",
        "Uma relação por linha\n",
        "\n",
        "Nenhum texto adicional antes ou depois\n",
        "\n",
        "Texto Exemplo:\n",
        "{text_few_shot}\n",
        "\n",
        "Relações de Exemplo: {fs}\n",
        "\n",
        "Texto a considerar: {test_text}\n",
        "\n",
        "Entidades anotadas: {getEntities2Consider(test_doc)}\n",
        "\n",
        "Relações:\n",
        "\n",
        "  '''\n",
        "  return prompt_relations"
      ],
      "metadata": {
        "id": "1YMGQGzOAICn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#insatll dependencies\n",
        "!pip install -U google-generativeai\n",
        "import google.generativeai as genai\n",
        "import os"
      ],
      "metadata": {
        "id": "5Hi4IGaRHdZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if you want to run jus the relations load the API Key here and uncomment the variable\n",
        "#LANGEXTRACT_API_KEY=\"\"\n",
        "\n",
        "def promptGEMINI(prompt):\n",
        "  genai.configure(api_key=LANGEXTRACT_API_KEY)\n",
        "\n",
        "  model = genai.GenerativeModel(\n",
        "      model_name=\"gemini-2.5-flash-lite\",\n",
        "      generation_config={\n",
        "          \"temperature\": 0.7,\n",
        "          \"top_p\": 0.95\n",
        "      }\n",
        "  )\n",
        "\n",
        "\n",
        "  response = model.generate_content(prompt)\n",
        "\n",
        "  # 4. Print the results\n",
        "  print(\"-\" * 30)\n",
        "  print(f\"Response:\\n{response.text}\")\n",
        "  print(\"-\" * 30)\n",
        "  return response"
      ],
      "metadata": {
        "id": "jsfrJS7nE7MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relations_results_path=\"\" #path to the result folder (where to put the generated results)\n",
        "\n",
        "\n",
        "#function to get the relations results from gemini given a test document and a few shot document\n",
        "\n",
        "def getRelationsFromGemini(test_doc,fs_doc=\"lusa_106\"):\n",
        "  with open(os.path.join(path_lusa,fs_doc+\".txt\"),\"r\") as f:\n",
        "    text_few_shot=f.read()\n",
        "  fs=getFewShotRelationExamples(fs_doc)\n",
        "\n",
        "  prompt=getRelationPrompt(text_few_shot=text_few_shot,fs=fs,test_doc=test_doc)\n",
        "  response=promptGEMINI(prompt)\n",
        "\n",
        "  filename=test_doc+\"rel.txt\"\n",
        "  with open(os.path.join(relations_results_path,filename), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(response.text)\n",
        "  return response\n",
        "\n"
      ],
      "metadata": {
        "id": "80ikB6r8FTCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RELATIONS FULL DATA"
      ],
      "metadata": {
        "id": "KzK3SDUuLcH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "#time.sleep(60*60)\n",
        "for test_doc in test_set_ids:\n",
        "  print(test_doc)\n",
        "  getRelationsFromGemini(test_doc)"
      ],
      "metadata": {
        "id": "7ytSVTKJLenq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Consistency Analysis"
      ],
      "metadata": {
        "id": "Bapc-x7sI3Pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not included in the paper but it allows an analysis on the coverage of the relation extraction process on the usage of the ner extracted. Simply put it, it allows to see how many entities were correctly mapped from the ner extraction to the relations based on exact span matching."
      ],
      "metadata": {
        "id": "z-rsztmqS1Cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relation_folder=\"\" # the relation results folder\n",
        "entities_folder=\"\" # the entities results folder"
      ],
      "metadata": {
        "id": "CdkW7JQqI5dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_to_json_line(line: str) -> str:\n",
        "    line = line.strip()\n",
        "    if not line:\n",
        "        return line\n",
        "\n",
        "    line = re.sub(r'([{\\s,])([A-Za-z_]\\w*)\\s*:', r'\\1\"\\2\":', line)\n",
        "\n",
        "\n",
        "    line = re.sub(\n",
        "        r'(\"span\"\\s*:\\s*)([^,}]+)',\n",
        "        lambda m: m.group(1) + json.dumps(m.group(2).strip(),ensure_ascii=False),\n",
        "        line\n",
        "    )\n",
        "\n",
        "    line = re.sub(r'(\"class\"\\s*:\\s*)([A-Za-z_]\\w*)', r'\\1\"\\2\"', line)\n",
        "\n",
        "    return line"
      ],
      "metadata": {
        "id": "pyK6ANYVLw5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import ast\n",
        "def getRelationResult(doc=\"lusa_100\",complete=False):\n",
        "  res=list()\n",
        "  with open(os.path.join(relation_folder,doc+\"rel.txt\")) as f:\n",
        "    lines = [line.rstrip() for line in f]\n",
        "  res=list()\n",
        "  for l in lines:\n",
        "    try:\n",
        "      d=fix_to_json_line(l)\n",
        "      d=json.loads(d)\n",
        "      d=dict(d)\n",
        "\n",
        "      if complete:\n",
        "        res.append(d)\n",
        "      else:\n",
        "        b=d[\"arg1\"][\"begin\"]\n",
        "\n",
        "        e=d[\"arg1\"][\"end\"]\n",
        "\n",
        "        span=d[\"arg1\"][\"span\"]\n",
        "        res.append((b,e,span))\n",
        "        b=d[\"arg2\"][\"begin\"]\n",
        "        e=d[\"arg2\"][\"end\"]\n",
        "        span=d[\"arg2\"][\"span\"]\n",
        "\n",
        "        res.append((b,e,span))\n",
        "    except Exception as e:\n",
        "      print(\"error \" + str(e))\n",
        "      print(l)\n",
        "\n",
        "  return res\n"
      ],
      "metadata": {
        "id": "4i6E-tPiJgf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count=0\n",
        "size=0\n",
        "for test_doc in test_set_ids:\n",
        "  ent=getEntities2Consider(test_doc,d=True)\n",
        "  rel=getRelationResult(test_doc)\n",
        "  size=size+len(rel)\n",
        "  for (b,e,span) in rel:\n",
        "    for entry in ent:\n",
        "      #print(entry)\n",
        "      if b==entry[\"begin\"] and e==entry[\"end\"]:\n",
        "        count=count+1\n",
        "        break"
      ],
      "metadata": {
        "id": "IIIiqOkBP4w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Relations linked with the extracted entities:\", count)\n",
        "print(\"Total Relations Extracted:\",size)\n",
        "print(\"Accuracy on correct Relation-Entity Link:\", count/size)\n",
        "\n"
      ],
      "metadata": {
        "id": "WrqMCPeKRHSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "ECR4UQZlZqnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation of the results"
      ],
      "metadata": {
        "id": "ZAaGxEGwTisI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "jbhuRm4kaBmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing functions\n",
        "test_doc=\"lusa_100\"\n",
        "predictions=getRelationResult(test_doc,complete=True)\n",
        "ground_truth=getFewShotRelationExamples(test_doc,json=True)\n"
      ],
      "metadata": {
        "id": "UXcb0n3OZuK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#util function for format conversion\n",
        "def convertJson2Tuple(list_gt):\n",
        "  res=list()\n",
        "  for ind in list_gt:\n",
        "    entry=(ind[\"class\"],str(ind[\"arg1\"][\"begin\"]),str(ind[\"arg1\"][\"end\"]),str(ind[\"arg2\"][\"begin\"]),str(ind[\"arg2\"][\"end\"]))\n",
        "    res.append(entry)\n",
        "  return res\n"
      ],
      "metadata": {
        "id": "z_4PH4xHCAiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute the metrics\n",
        "def computeGTandPredBetter(ground_truth,predictions):\n",
        "\n",
        "  list_gt=list()\n",
        "  list_pred=list()\n",
        "  total_tp = total_fp = total_fn = 0\n",
        "  results=dict()\n",
        "\n",
        "\n",
        "  labels=[\"TLINK\",\"OLINK\",\"SRLINK\",\"QSLINK\"]\n",
        "  counts = {label: {\"tp\": 0, \"fp\": 0, \"fn\": 0,\"sup\":0} for label in labels}\n",
        "\n",
        "  for test_doc in test_set_ids:\n",
        "    predictions=getRelationResult(test_doc,complete=True)\n",
        "    ground_truth=getFewShotRelationExamples(test_doc,json=True)\n",
        "    predictions=set(convertJson2Tuple(predictions))\n",
        "    ground_truth=set(convertJson2Tuple(ground_truth))\n",
        "\n",
        "\n",
        "\n",
        "    for label in labels:\n",
        "        #filter by label\n",
        "        pred_label = {ann for ann in predictions if ann[0] == label}\n",
        "        gold_label = {ann for ann in ground_truth if ann[0] == label}\n",
        "\n",
        "        counts[label][\"tp\"] += len(pred_label & gold_label)\n",
        "        counts[label][\"fp\"] += len(pred_label - gold_label)\n",
        "        counts[label][\"fn\"] += len(gold_label - pred_label)\n",
        "        counts[label][\"sup\"] += len(gold_label)\n",
        "\n",
        "\n",
        "\n",
        "  for label in labels:\n",
        "    tp, fp, fn = counts[label][\"tp\"], counts[label][\"fp\"], counts[label][\"fn\"]\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "    results[label] = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "    total_tp += tp\n",
        "    total_fp += fp\n",
        "    total_fn += fn\n",
        "\n",
        "       # Micro-average across both labels\n",
        "  micro_p = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
        "  micro_r = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
        "  micro_f1 = 2 * micro_p * micro_r / (micro_p + micro_r) if (micro_p + micro_r) > 0 else 0.0\n",
        "\n",
        "  results[\"micro_avg\"] = {\"precision\": micro_p, \"recall\": micro_r, \"f1\": micro_f1}\n",
        "  print(list(predictions)[0],list(ground_truth)[0])\n",
        "  return results"
      ],
      "metadata": {
        "id": "tGYoE8OgBVo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results=computeGTandPredBetter(ground_truth, predictions)\n"
      ],
      "metadata": {
        "id": "10M7u9fsETLV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
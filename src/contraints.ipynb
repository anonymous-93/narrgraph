{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse brat file to memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#is cuda availabe\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brat_parser import get_entities_relations_attributes_groups\n",
    "\n",
    "def parseBratFile(file):\n",
    "    entities, relations, attributes, groups = get_entities_relations_attributes_groups(file)\n",
    "    #filter movelink relations\n",
    "    relations = {id: rel for id, rel in relations.items() if \"MOVELINK\" not in rel.type}\n",
    "    return entities, relations, attributes, groups\n",
    "# The praser returns dataclasses.\n",
    "\n",
    "\n",
    "def filter_annotations(annotations, type, match=False, exceptions=[]):\n",
    "    if match:\n",
    "        return {id: ann for id, ann in annotations.items() if type in ann.type and ann.type not in exceptions}\n",
    "    else:\n",
    "        return {id: ann for id, ann in annotations.items() if ann.type == type and ann.type not in exceptions}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83, 19, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "path = \"./lusa_news\"\n",
    "\n",
    "\n",
    "test_set_ids = ['lusa_97',\n",
    " 'lusa_4',\n",
    " 'lusa_67',\n",
    " 'lusa_20',\n",
    " 'lusa_83',\n",
    " 'lusa_104',\n",
    " 'lusa_80',\n",
    " 'lusa_79',\n",
    " 'lusa_34',\n",
    " 'lusa_47',\n",
    " 'lusa_30',\n",
    " 'lusa_96',\n",
    " 'lusa_11',\n",
    " 'lusa_112',\n",
    " 'lusa_100',\n",
    " 'lusa_77',\n",
    " 'lusa_38',\n",
    " 'lusa_86',\n",
    " 'lusa_60']\n",
    "\n",
    "\n",
    "val_set_ids = ['lusa_12',\n",
    " 'lusa_48',\n",
    " 'lusa_45',\n",
    " 'lusa_115',\n",
    " 'lusa_44',\n",
    " 'lusa_82',\n",
    " 'lusa_111',\n",
    " 'lusa_63',\n",
    " 'lusa_8',\n",
    " 'lusa_42',\n",
    " 'lusa_50',\n",
    " 'lusa_76',\n",
    " 'lusa_114',\n",
    " 'lusa_99',\n",
    " 'lusa_18']\n",
    "\n",
    "#test_set_ids = ['lusa_4']\n",
    "\n",
    "files_train = []\n",
    "files_test = []\n",
    "files_val = []\n",
    "#exceptions = [\"lusa_52.txt\", \"lusa_88.txt\", \"lusa_22.txt\"] # n têm SRLinks | lusa_113 flaggeed, no ha tlinks entre eventos e Times? \n",
    "for file in os.listdir(path):\n",
    "    if file.endswith(\".txt\") and file.startswith(\"lusa\") and file: #not in exceptions:\n",
    "        file = re.match(r'(.*)\\.txt',file)\n",
    "        if file[1] in test_set_ids:\n",
    "            files_test.append(os.path.join(path, file[1]))\n",
    "        elif file[1] in val_set_ids:\n",
    "            files_val.append(os.path.join(path, file[1]))\n",
    "        else:\n",
    "            files_train.append(os.path.join(path, file[1]))\n",
    "        \n",
    "len(files_train), len(files_test), len(files_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "import torch\n",
    "nlp = spacy.load('pt_core_news_lg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_relation_by_entity(relations, entities, link_type_prefix, entity_types={\"Event\", \"Time\"}):\n",
    "    filtered = {}\n",
    "    discarded = {}\n",
    "    for rid, rel in relations.items():\n",
    "        # Only TLINK relations\n",
    "        if not rel.type.startswith(link_type_prefix):\n",
    "            continue\n",
    "\n",
    "        subj_ent = entities.get(rel.subj)\n",
    "        obj_ent = entities.get(rel.obj)\n",
    "\n",
    "        # Skip if entities are missing\n",
    "        if subj_ent is None or obj_ent is None:\n",
    "            continue\n",
    "\n",
    "        # Event–Time or Time–Event\n",
    "        types = {subj_ent.type, obj_ent.type}\n",
    "        if types == entity_types:\n",
    "            rel.type = \"event-\" + rel.type\n",
    "            filtered[rid] = rel\n",
    "            \n",
    "        else:\n",
    "            discarded[rid] = rel\n",
    "\n",
    "    return filtered, discarded\n",
    "\n",
    "def parse_brat(files):\n",
    "    entities_list = []\n",
    "    relations_list = []\n",
    "    texts_list = []\n",
    "    for file in files:\n",
    "        #file = \"../../Datasets/lusa_news_final/lusa_1\"\n",
    "        entities, relations, attributes, _ = parseBratFile(file + \".ann\")\n",
    "        #tlinks_event, tlinks_other = filter_relation_by_entity(relations, entities, \"TLINK_\", entity_types={\"Event\", \"Time\"})\n",
    "        events = filter_annotations(entities, \"Event\")\n",
    "        times = filter_annotations(entities, \"Time\")\n",
    "        spatial_relations = filter_annotations(entities, \"Spatial_Relation\")\n",
    "        participants = filter_annotations(entities, \"Participant\")\n",
    "        #entities_ = events | participants | times | spatial_relations\n",
    "        entities_ = events | participants | times | spatial_relations\n",
    "        tlinks =  filter_annotations(relations, \"TLINK\", match=True)\n",
    "        qslinks =  filter_annotations(relations, \"QSLINK\", match=True)\n",
    "        olink =  filter_annotations(relations, \"OLINK\", match=True)#, exceptions=[\"OLINK_objIdentity\"])\n",
    "        mlink =  filter_annotations(relations, \"MOVELINK\", True)\n",
    "        srl_links = filter_annotations(relations, \"SRL\", match=True)\n",
    "        #relations_ = tlinks | qslinks | olink | mlink | srl_links\n",
    "        relations_ = srl_links |  qslinks | olink | tlinks #tlinks_event | tlinks_other #tlinks #\n",
    "        text = open(file + \".txt\").read()\n",
    "        entities_list.append(entities_)\n",
    "        relations_list.append(relations_)\n",
    "        texts_list.append(text)\n",
    "    return entities_list, relations_list, texts_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Node Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lfc/.myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "import tokenizations\n",
    "\n",
    "\n",
    "\n",
    "entity_labels = [\"O\", \"Event\", \"Participant\", \"Time\", \"Spatial_Relation\"] #,\"Time\",\"Spatial_Relation\"]\n",
    "iob_labels = [\"O\", \"B-Event\", \"I-Event\", \"B-Participant\", \"I-Participant\", \"B-Time\", \"I-Time\", \"B-Spatial_Relation\", \"I-Spatial_Relation\"]\n",
    "label2id = {label: i  for i, label in enumerate(entity_labels)}\n",
    "iob2id = {label: i  for i, label in enumerate(iob_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "id2iob = {i: label for label, i in iob2id.items()}\n",
    "universal_pos_tags = [\"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"CONJ\", \"CCONJ\", \"DET\",\n",
    "                      \"INTJ\", \"NOUN\", \"NUM\", \"PART\", \"PRON\", \"PROPN\",\n",
    "                      \"PUNCT\", \"SCONJ\", \"SYM\", \"VERB\", \"X\", \"SPACE\"]\n",
    "\n",
    "\n",
    "#One Hot Encoding para labels de entidades / dep_parser labels\n",
    "def one_hot_encode(index, size):\n",
    "    one_hot = torch.zeros(size, dtype=torch.float)\n",
    "    one_hot[index] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "def get_spacy_token_indices(span, doc, idx):\n",
    "        start, end = span[0], span[1]\n",
    "        #print(start, end)\n",
    "        \n",
    "        token_indices = []\n",
    "        for token in doc:\n",
    "            #print(token)\n",
    "            if token.idx >= start and token.idx + len(token.text) <= end: # rever...\n",
    "                token_indices.append(token.i)\n",
    "            elif token.idx >= start and token.idx <= end <= token.idx + len(token.text):\n",
    "                token_indices.append(token.i)\n",
    "            elif token.idx <= start  <= token.idx + len(token.text) and token.idx + len(token.text) <= end:\n",
    "                token_indices.append(token.i)\n",
    "        if token_indices == []:\n",
    "            \n",
    "            print(\"Error when mapping spacy tokens to entities spans\")\n",
    "            print(span)\n",
    "            print(doc.text[start:end])\n",
    "            print(doc.text[start- 10:end+10])\n",
    "            print(doc)\n",
    "            print(\"Entity ID:\", idx)\n",
    "            for token in doc:\n",
    "                print(token.idx, len(token.text))\n",
    "                print(token.text)\n",
    "        return token_indices\n",
    "\n",
    "# Calcular a média dos embeddings dos tokens de uma entidade\n",
    "def aggregate_embeddings(indices, embeddings):\n",
    "    if indices:\n",
    "        \n",
    "        return torch.mean(embeddings[indices], dim=0)\n",
    "    else:\n",
    "        print(\"error calculating entity embeddings\")\n",
    "        #print(indices)\n",
    "        #print(entity_indices)\n",
    "        return torch.zeros(embeddings.size(1))\n",
    "\n",
    "\n",
    "\n",
    "def entity_indices_to_iob(n_tokens, entity_indices):\n",
    "    iob_tags = [iob2id['O']] * n_tokens  # default all outside\n",
    "\n",
    "    for token_indices, ent_type_idx, _ in entity_indices:\n",
    "        ent_type = id2label[ent_type_idx]\n",
    "        if ent_type != \"O\":\n",
    "            for i, token_idx in enumerate(token_indices):\n",
    "                prefix = 'B' if i == 0 else 'I'\n",
    "                iob = f\"{prefix}-{ent_type}\"\n",
    "                iob_tags[token_idx] = iob2id[iob]\n",
    "    return iob_tags\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generateEntityLabels(doc, entity_indices):\n",
    "    n_tokens = len(doc)\n",
    "    labels = entity_indices_to_iob(n_tokens, entity_indices)\n",
    "    # generate one-hot encoding for entity labels\n",
    "    entity_type_one_hot = torch.stack([one_hot_encode(label, len(iob_labels)) for label in labels])\n",
    "    return labels, entity_type_one_hot\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('neuralmind/bert-base-portuguese-cased') ## neuralmind/bert-base-portuguese-cased\n",
    "\n",
    "def tokenize_and_align_labels(tokenized_inputs, labels):\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "        # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "        # ignored in the loss function.\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        # We set the label for the first token of each word.\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(labels[word_idx])\n",
    "        # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "        # the label_all_tokens flag.\n",
    "        else:\n",
    "            label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized_inputs[\"labels\"] = torch.tensor([label_ids])\n",
    "    return tokenized_inputs        \n",
    "\n",
    "def generate_bert_encodings(doc, labels):\n",
    "    tokens = [token.text for token in doc]\n",
    "    encodings = tokenizer(tokens, is_split_into_words=True, return_tensors='pt', max_length=512, truncation=True, padding='max_length', add_special_tokens=True)\n",
    "    encodings = tokenize_and_align_labels(encodings,labels)\n",
    "    \n",
    "    tokens_b = tokenizer.convert_ids_to_tokens(encodings[\"input_ids\"][0])\n",
    "    tokens_b = [tok for tok in tokens_b if tok != tokenizer.pad_token]\n",
    "    a2b, _ = tokenizations.get_alignments(tokens, tokens_b)\n",
    "    encodings['alignment'] = a2b\n",
    "    encodings['pos_vectors'] = [one_hot_encode(universal_pos_tags.index(token.pos_), len(universal_pos_tags)) for token in doc]\n",
    "    return encodings\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate input Edge_Index with the dependency parser tree (graph connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_one_hot(edge_dict, edge, one_hot_attr):\n",
    "    if edge not in edge_dict:\n",
    "        edge_dict[edge] = one_hot_attr\n",
    "    else:\n",
    "        edge_dict[edge] = torch.logical_or(edge_dict[edge].bool(), one_hot_attr.bool()).float() # Accumulate one-hot vectors\n",
    "    return edge_dict\n",
    "\n",
    "\n",
    "def generateEdgeIndex(doc, input_rel_label2id, strategy=[\"dep\", \"seq\"]):\n",
    "    # Create the dependency graph between entities\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    unique_edges = []\n",
    "    n_tokens = len(doc)\n",
    "    edges_dict = {}\n",
    "    #token_to_entity_idx = generateTokenToEntityIndex(entity_indices)\n",
    "    if \"dep\" in strategy:  \n",
    "        for token in doc:\n",
    "            if token.head != token: #and token.i in token_to_entity_idx and token.head.i in token_to_entity_idx: # se o token e o head forem entidades\n",
    "                edge = (token.i, token.head.i)\n",
    "                #reverse = (token_to_entity_idx[token.head.i], token_to_entity_idx[token.i])\n",
    "                one_hot_attr = one_hot_encode(input_rel_label2id[token.dep_], len(input_rel_label2id)) #\n",
    "                if edge[0] != edge[1]:\n",
    "                    edges_dict = accumulate_one_hot(edges_dict, edge, one_hot_attr)\n",
    "            \n",
    "    if \"seq\" in strategy:\n",
    "        for i in range(n_tokens - 1):\n",
    "            edge = (i, i + 1)\n",
    "            one_hot_attr = one_hot_encode(input_rel_label2id[\"seq\"], len(input_rel_label2id ))\n",
    "            edges_dict = accumulate_one_hot(edges_dict, edge, one_hot_attr)\n",
    "\n",
    "    if \"full\" in strategy:\n",
    "        for i in range(n_tokens):\n",
    "            for j in range(i + 1, n_tokens):\n",
    "                edge = (i, j)\n",
    "                one_hot_attr = one_hot_encode(input_rel_label2id[\"full\"], len(input_rel_label2id))\n",
    "                edges_dict = accumulate_one_hot(edges_dict, edge, one_hot_attr)\n",
    "    \n",
    "    edge_index = torch.tensor(list(edges_dict.keys()), dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.stack(list(edges_dict.values()))\n",
    "\n",
    "    #print(\"Edge index:\", edge_index.shape)\n",
    "    return edge_index, edge_attr\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate output Output Edge_Index with LUSA relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = []\n",
    "def generateTargetEdgeIndex(relations, node_id_mapping, relation_type_mapping):\n",
    "    edge_indices = []\n",
    "    edge_dict = {}\n",
    "    edge_attr_text_dict = {} \n",
    "    global k\n",
    "    edge_attributes = []\n",
    "    edge_attr_text = []\n",
    "    for rel in relations.values():\n",
    "        #print(rel)\n",
    "        if rel.subj in node_id_mapping and rel.obj in node_id_mapping:\n",
    "            subj_idx = node_id_mapping[rel.subj]\n",
    "            obj_idx = node_id_mapping[rel.obj]\n",
    "            if subj_idx == obj_idx:\n",
    "                print(\" Self-loop detected\")\n",
    "                print(rel)\n",
    "            edge_key = tuple(sorted([subj_idx, obj_idx]))\n",
    "            #one_hot_attr = torch.tensor(one_hot_encode(relation_type_mapping[rel.type.split(\"_\")[0]],len(relation_type_mapping))).clone().detach()\n",
    "            one_hot_attr = torch.as_tensor(one_hot_encode(relation_type_mapping[rel.type.split(\"_\")[0]], len(relation_type_mapping))).clone().detach() #rel.type.split(\"_\")[0]\n",
    "\n",
    "   \n",
    "                # If the edge exists, sum the one-hot encoded attributes\n",
    "            if edge_key in edge_dict:\n",
    "                \n",
    "                edge_dict[edge_key] = torch.logical_or(edge_dict[edge_key].bool(), one_hot_attr.bool()).float() # Accumulate one-hot vectors \n",
    "                edge_attr_text_dict[edge_key].add(rel.type.split(\"_\")[0])  # Store unique text labels\n",
    "            else:\n",
    "                edge_dict[edge_key] = one_hot_attr\n",
    "                edge_attr_text_dict[edge_key] = {rel.type.split(\"_\")[0]}\n",
    "        else:\n",
    "            print(rel)\n",
    "            print(\"ERROR: Relation refers to missing nodes\")\n",
    "    # Convert the dictionary into tensors\n",
    "# Convert the dictionary into tensors\n",
    "    if len(edge_dict) > 0:\n",
    "        edge_indices_y = torch.tensor(list(edge_dict.keys()), dtype=torch.long).t().contiguous()\n",
    "        edge_attr_y = torch.stack(list(edge_dict.values()))\n",
    "    else:\n",
    "        edge_indices_y = torch.empty((2, 0), dtype=torch.long)\n",
    "        edge_attr_y = torch.empty((0, len(relation_type_mapping)), dtype=torch.float)\n",
    "\n",
    "    edge_attr_text = [\"|\".join(sorted(types)) for types in edge_attr_text_dict.values()]\n",
    "    \n",
    "    return edge_indices_y, edge_attr_y, edge_attr_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a input and output graph given a Lusa document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_entity_preds(filename):\n",
    "    entity_preds = json.load(open(filename))\n",
    "    #one_hot_encode entity preds\n",
    "    entity_preds_one_hot = []\n",
    "    for batch in entity_preds:\n",
    "        batch_preds = []\n",
    "        for pred in batch:\n",
    "            batch_preds.append(one_hot_encode(iob2id[pred], len(iob_labels)))\n",
    "        entity_preds_one_hot.append(batch_preds)\n",
    "    return entity_preds_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# Create a mapping from token indices to entity indices\n",
    "def generateTokenToEntityIndex(entity_indices):\n",
    "    token_to_entity_idx = {}\n",
    "    entity_sizes = {i: len(indices) for i, (indices, _, _) in enumerate(entity_indices)}\n",
    "    discarded_entities = set()\n",
    "    for entity_idx, (indices, type, idx) in enumerate(entity_indices):\n",
    "        for token_idx in indices:\n",
    "            #print(token_idx)\n",
    "            # If token already assigned, keep the entity with more tokens\n",
    "            if token_idx in token_to_entity_idx:\n",
    "                current_entity = token_to_entity_idx[token_idx]\n",
    "                if entity_sizes[entity_idx] > entity_sizes[current_entity]:\n",
    "                    discarded_entities.add(entity_indices[current_entity][2])  # Track discarded entity\n",
    "                    token_to_entity_idx[token_idx] = entity_idx\n",
    "                else:\n",
    "                    discarded_entities.add(idx)  # Track the smaller entity\n",
    "            else:\n",
    "                token_to_entity_idx[token_idx] = entity_idx\n",
    "    return token_to_entity_idx, discarded_entities\n",
    "\n",
    "def getGroupedTokens(doc, entity_indices):\n",
    "    grouped_tokens = []\n",
    "    visited_tokens = set()\t\n",
    "    indices_entity, discarded_entities = generateTokenToEntityIndex(entity_indices)\n",
    "    for token in doc:\n",
    "        if token.i not in indices_entity:\n",
    "            grouped_tokens.append(([token.i], 0, \"\")) # out of entity index: 0\n",
    "        else:\n",
    "            entity_idx = indices_entity[token.i]\n",
    "            if entity_idx not in visited_tokens:\n",
    "                grouped_tokens.append(entity_indices[entity_idx])\n",
    "                visited_tokens.add(entity_idx)\n",
    "    return grouped_tokens, discarded_entities\n",
    "\n",
    "def generateDataset(entities, relations, text, relation_type_mapping, input_rel_label2id):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\xa0\", \" \")\n",
    "    text = text.replace(\"´\", \"'\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    entity_indices = [(get_spacy_token_indices(entity.span[0], doc, idx), label2id[entity.type], idx) for idx, entity in entities.items()]\n",
    "    #print(entity_indices)\n",
    "    \n",
    "    labels, label_one_hot = generateEntityLabels(doc, entity_indices)\n",
    "    tokens = [token.text for token in doc]\n",
    "    encodings = generate_bert_encodings(doc,labels)\n",
    "\n",
    "    iob_labels = [id2iob[label] for label in labels]\n",
    "\n",
    "    edge_index, edge_attr = generateEdgeIndex(doc, input_rel_label2id)\n",
    " \n",
    "    node_id_mapping = {node_id: indices[0] for idx, (indices,type,node_id) in enumerate(entity_indices)}\n",
    "    y_edge_index, y_edge_attr, edge_attr_text = generateTargetEdgeIndex(relations, node_id_mapping, relation_type_mapping)\n",
    "\n",
    "\n",
    "    data = Data(\n",
    "        x=torch.zeros(len(tokens)), \n",
    "        edge_index=edge_index, edge_attr=edge_attr, \n",
    "        y=torch.tensor(labels), \n",
    "        encodings = encodings, \n",
    "        target_edge_index=y_edge_index, \n",
    "        y_edge_attr=y_edge_attr, \n",
    "        tokens= tokens,\n",
    "        labels = torch.tensor(labels), \n",
    "        iob_labels = iob_labels, \n",
    "        label_one_hot = torch.tensor(label_one_hot),\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate list of graphs for each document of the Lusa Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "relation_type_mapping = {'TLINK': 0, 'SRLINK': 1, 'QSLINK': 2, \"OLINK\": 3}#, \"event-TLINK\":4}\n",
    "\n",
    "def generateDatasets(files):\n",
    "    data_list = []\n",
    "    entities_list, relations_list, texts_list = parse_brat(files)\n",
    "    print(len(entities_list), len(relations_list), len(texts_list))\n",
    "    \n",
    "\n",
    "    print(relation_type_mapping)\n",
    "    input_rel_label2id = {label: idx for idx, label in enumerate(list(nlp.get_pipe(\"parser\").labels) + [\"seq\", \"full\", \"entity\"])}\n",
    "\n",
    "    for i, entities in enumerate(entities_list):\n",
    "\n",
    "        data = generateDataset(entities, relations_list[i], texts_list[i], relation_type_mapping,input_rel_label2id)\n",
    "        data['file_name'] = files[i]\n",
    "\n",
    "        data_list.append(data)\n",
    "\n",
    "    return data_list\n",
    "\n",
    "data_list_train = generateDatasets(files_train)\n",
    "data_list_test = generateDatasets(files_test)\n",
    "data_list_val = generateDatasets(files_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_preds = get_entity_preds(\"lusa_ner_predictions_test.json\")\n",
    "for data, preds in zip(data_list_test, entity_preds):\n",
    "    data.entity_preds = torch.stack(preds)\n",
    "entity_preds = get_entity_preds(\"lusa_ner_predictions_val.json\")\n",
    "for data, preds in zip(data_list_val, entity_preds):\n",
    "    data.entity_preds = torch.stack(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(data_list_train, batch_size=1, shuffle=False)\n",
    "val_loader = DataLoader(data_list_val, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(data_list_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definning the Graph Auto-Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import TransformerConv\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch.nn import BatchNorm1d\n",
    "from transformers import BertModel, AutoConfig, BertTokenizer\n",
    "from torch_geometric.nn import BatchNorm\n",
    "import random\n",
    "import torch\n",
    "import tokenizations\n",
    "import numpy as np\n",
    "\n",
    "def set_all_seeds(seed=42):\n",
    "    import os\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Deterministic CuDNN\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "    # Optional: environment variable for further determinism\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\n",
    "set_all_seeds() \n",
    "\n",
    "class GNNForEdgePrediction(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, config):\n",
    "        super(GNNForEdgePrediction, self).__init__()\n",
    "        decoder_size = 512\n",
    "        dep_size = len(nlp.get_pipe(\"parser\").labels) + 3\n",
    "  \n",
    "        self.num_entity_labels = len(iob_labels)\n",
    "        num_relation_labels = len(relation_type_mapping)\n",
    "\n",
    "        # Encoder Layers\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.conv1 = GATv2Conv(input_dim, hidden_dim, heads=4, concat=False, edge_dim=dep_size)\n",
    "        self.conv2 = GATv2Conv(hidden_dim, hidden_dim, heads=4, concat=False,  edge_dim=dep_size)\n",
    "        self.conv3 = GATv2Conv(hidden_dim, hidden_dim, heads=4, concat=False,  edge_dim=dep_size)\n",
    "\n",
    "        # Latent Layers (GVAE)\n",
    "        #self.conv_mu = TransformerConv(hidden_dim, hidden_dim, heads=4, concat=False,  edge_dim=dep_size)\n",
    "        #self.conv_logstd = TransformerConv(hidden_dim, hidden_dim, heads=4, concat=False,  edge_dim=dep_size)\n",
    "\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_dense_1 = nn.Linear(hidden_dim*2 + self.num_entity_labels*2, decoder_size)\n",
    "        self.decoder_dense_2 = nn.Linear(decoder_size, decoder_size)\n",
    "        self.decoder_dense_3 = nn.Linear(decoder_size, decoder_size)\n",
    "        self.label_pred_layer = nn.Linear(decoder_size, len(relation_type_mapping)) #srlinks and tlinks | qslinks\n",
    "        self.entity_classifier = nn.Linear(hidden_dim, self.num_entity_labels)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "  # Relation classifier dropout\n",
    "        self.relation_dropout = nn.Dropout(classifier_dropout)\n",
    "\n",
    "        # Fusion + final relation decoders\n",
    "        fusion_input_dim = hidden_dim * 2 + 2 * self.num_entity_labels  # relation_feat + source_ent + target_ent\n",
    "        self.fusion_layer = nn.Linear(fusion_input_dim, hidden_dim)\n",
    "        self.relation_classifier_final = nn.Linear(hidden_dim, num_relation_labels)\n",
    "\n",
    "        # Final GNN to refine entities using relations\n",
    "        self.relation_to_entity_gnn = GATv2Conv(hidden_dim, hidden_dim, heads=4, concat=False,  edge_dim=hidden_dim * 2)\n",
    "        self.entity_classifier_final = nn.Linear(hidden_dim, self.num_entity_labels)\n",
    "        \n",
    "    def encode(self, x, edge_index, edge_attr):\n",
    "\n",
    "        h1 = self.conv1(x, edge_index, edge_attr)\n",
    "        #h2 = self.conv2(h1, edge_index, edge_attr) + h1\n",
    "        #h3 = self.conv3(h2, edge_index, edge_attr) + h2\n",
    "\n",
    "        return h1\n",
    "\n",
    "        return z_mu, z_logstd\n",
    "  \n",
    "    def preprocess_bert(self, data):\n",
    "        tokens = data.tokens\n",
    "        encodings = data.encodings\n",
    "\n",
    "        bert_output = self.bert(encodings[\"input_ids\"], attention_mask=encodings[\"attention_mask\"], token_type_ids=encodings[\"token_type_ids\"])\n",
    "        bert_embeddings = bert_output.last_hidden_state#.squeeze()\n",
    "        all_embeddings = []\n",
    "        #print(\"BERT embeddings shape:\", bert_embeddings.shape)\n",
    "        all_embeddings = []\n",
    "        for i, indices_list in enumerate(encodings['alignment']):\n",
    "            z_embeddings = []\n",
    "            for j, indices in enumerate(indices_list):\n",
    "                if indices:\n",
    "                    token_embedding = bert_embeddings[i, indices[0], :]\n",
    "                else:\n",
    "                    token_embedding = torch.zeros(bert_embeddings.shape[2]).to(bert_embeddings.device)\n",
    "\n",
    "                # Concatenate POS one-hot vector\n",
    "                pos_vec = encodings['pos_vectors'][j].to(token_embedding.device)\n",
    "                token_embedding = torch.cat([token_embedding, pos_vec], dim=-1)\n",
    "\n",
    "                z_embeddings.append(token_embedding)\n",
    "\n",
    "            all_embeddings.append(torch.stack(z_embeddings))\n",
    "        \n",
    "        return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "\n",
    "    def decode_joint(self, z, entity_logits_init):\n",
    "\n",
    "        graph_z = z\n",
    "        #print(\"Graph Z shape:\", graph_z.shape)\n",
    "        edge_indicesX = torch.triu_indices(graph_z.shape[0], graph_z.shape[0], offset=1) # calculate upper part of the matrix indexes\n",
    "        #print(\"Edge indicesX shape:\", edge_indicesX.shape)\n",
    "        source_indices = edge_indicesX[0]\n",
    "        #print(source_indices.shape)\n",
    "        target_indices = edge_indicesX[1]\n",
    "\n",
    "        source_features = graph_z[source_indices]\n",
    "        target_features = graph_z[target_indices]\n",
    "\n",
    "        entity_source = entity_logits_init[source_indices]\n",
    "        entity_target = entity_logits_init[target_indices]\n",
    "\n",
    "        source_features_joint = torch.cat([source_features, entity_source], dim=1)\n",
    "        target_features_joint = torch.cat([target_features, entity_target], dim=1)\n",
    "        #print(source_features.shape)\n",
    "        edge_features_joint = torch.cat([source_features_joint, target_features_joint], axis=1)\n",
    "        graph_inputs = torch.cat([source_features, target_features], axis=1)\n",
    "\n",
    "        k = self.fusion_layer(edge_features_joint).relu()\n",
    "        edge_logits_joint = self.relation_classifier_final(k)\n",
    "        return  edge_logits_joint #edge_indicesX #graph_inputs\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def decode(self, z):\n",
    "\n",
    "        graph_z = z\n",
    "\n",
    "        edge_indicesX = torch.triu_indices(graph_z.shape[0], graph_z.shape[0], offset=1) # calculate upper part of the matrix indexes\n",
    "\n",
    "        source_indices = edge_indicesX[0]\n",
    "\n",
    "        target_indices = edge_indicesX[1]\n",
    "\n",
    "        source_features = graph_z[source_indices]\n",
    "        target_features = graph_z[target_indices]\n",
    "\n",
    "        graph_inputs = torch.cat([source_features, target_features], axis=1)\n",
    "        # Get predictions\n",
    "        x = self.decoder_dense_1(graph_inputs).relu()\n",
    "        x = self.decoder_dense_2(x).relu()\n",
    "\n",
    "        x = self.relation_dropout(x)\n",
    "        edge_logits = self.label_pred_layer(x)\n",
    "        return  edge_logits #edge_indicesX #graph_inputs\n",
    "\n",
    "\n",
    "        \n",
    "    def reparameterize(self, mu, logstd):\n",
    "        if self.training:\n",
    "            #getting the std deviation\n",
    "            std = torch.exp(logstd)\n",
    "            #generate a random number from a normal distribution\n",
    "            eps = torch.randn_like(std)\n",
    "            #return the z value\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            #return the mean\n",
    "            return mu\n",
    "\n",
    "    def forward(self, data, pipeline=False):\n",
    "        data = data.to(device)\n",
    "        x , edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        x = self.preprocess_bert(data)\n",
    " \n",
    "\n",
    "        z = self.encode(x, edge_index, edge_attr) #mu, logstd\n",
    "\n",
    "        sequence_output = self.dropout(z)\n",
    "        entity_logits_init = self.entity_classifier(sequence_output)\n",
    "        #print(\"Entity logits init shape:\", entity_logits_init.shape)\n",
    "        #enitity logits to one hot\n",
    "\n",
    "        if self.training:\n",
    "\n",
    "            entity_features_detached = entity_logits_init.detach()\n",
    "        else:\n",
    "\n",
    "            if pipeline:\n",
    "                entity_features_detached = data.entity_preds.to(device)\n",
    "                print(\"using predicted entity labels\")\n",
    "            else:\n",
    "                entity_features_detached = entity_logits_init.detach()\n",
    "\n",
    "        \n",
    "        z = torch.cat([z, entity_features_detached], dim=1) #data.label_one_hot # entity_one_hot\n",
    "        \n",
    "\n",
    "        edge_logits = self.decode(z)\n",
    "        return entity_logits_init, edge_logits\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"./models/__joint_best_pos55.pt\"\n",
    "input_dim = 768 + len(universal_pos_tags) #768 + 5 BERT // 300 + 5 spacy\n",
    "hidden_dim = 768 + len(universal_pos_tags)#  512 spacy // 1024 BERT\n",
    "model_checkpoint = \"neuralmind/bert-base-portuguese-cased\"  # or any other model checkpoint\n",
    "config = AutoConfig.from_pretrained(model_checkpoint) # lfcc/lusa_events neuralmind/bert-base-portuguese-cased\n",
    "config.num_labels = len(iob_labels)  # Set the number of labels for token classification\n",
    "config.id2label = id2iob\n",
    "config.label2id = iob2id\n",
    "bert = BertModel.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    config=config,\n",
    "    add_pooling_layer=True  # Required to match pretrained checkpoint\n",
    ")\n",
    "model = GNNForEdgePrediction(input_dim, hidden_dim, config)\n",
    "missing_keys, unexpected_keys = model.bert.load_state_dict(\n",
    "    bert.state_dict(), strict=False\n",
    ")\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(model_save_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_predicted_edges(edge_preds, edge_src_types, edge_tgt_types, ignore_O=True, ignore_I=True):\n",
    "\n",
    "    masked_preds = edge_preds.copy()\n",
    "    x = 0\n",
    "    for i, (src, tgt) in enumerate(zip(edge_src_types, edge_tgt_types)):\n",
    "        #print(src, tgt)\n",
    "        if ignore_O and (id2iob[src] == 'O' or id2iob[tgt] == 'O'):\n",
    "            x = x + 1\n",
    "            masked_preds[i, :] = 0\n",
    "            #print(\"masked O\")\n",
    "        if ignore_I and (id2iob[src].startswith('I') or id2iob[tgt].startswith('I')):\n",
    "            masked_preds[i, :] = 0\n",
    "            #print(\"masked I\")\n",
    "    print(f\"Masked {x} edges\")\n",
    "    return masked_preds\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def refine_entities_with_relations(entity_logits, edge_index, edge_logits, topk=1):\n",
    "\n",
    "    num_nodes, num_entity_types = entity_logits.shape\n",
    "    refined_entity_logits = entity_logits.clone()\n",
    "\n",
    "    # 1. Compute edge probabilities\n",
    "    edge_probs = torch.sigmoid(edge_logits)  # [num_edges, num_relation_types]\n",
    "    # Keep only topk relations per edge\n",
    "    topk_vals, topk_idx = torch.topk(edge_probs, k=topk, dim=-1)\n",
    "\n",
    "    # 2. Message passing: propagate relation info to nodes\n",
    "    for i, (src, tgt) in enumerate(edge_index.t()):\n",
    "        for k in range(topk):\n",
    "            rel_prob = topk_vals[i, k]\n",
    "            # Heuristic: if edge exists with high probability, boost non-O predictions\n",
    "            if rel_prob > 0.5:\n",
    "                # Apply a small boost to entity logits of connected nodes\n",
    "                refined_entity_logits[src] += 0.1 * rel_prob\n",
    "                refined_entity_logits[tgt] += 0.1 * rel_prob\n",
    "\n",
    "    return refined_entity_logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def refine_entity_logits_with_relations(\n",
    "    entity_logits,\n",
    "    edge_probs,\n",
    "    edge_indices,\n",
    "    rel_threshold=0.5,\n",
    "    penalty=5.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Suppress O-label for tokens involved in confident relations.\n",
    "    \"\"\"\n",
    "    refined_logits = entity_logits.clone()\n",
    "\n",
    "    # Identify edges with at least one confident relation\n",
    "    confident_edges = (edge_probs > rel_threshold).any(dim=1)\n",
    "\n",
    "    if confident_edges.sum() == 0:\n",
    "        return refined_logits\n",
    "\n",
    "    src = edge_indices[0][confident_edges]\n",
    "    tgt = edge_indices[1][confident_edges]\n",
    "    involved_nodes = torch.unique(torch.cat([src, tgt]))\n",
    "\n",
    "    # Penalize O logit\n",
    "    refined_logits[involved_nodes, label2id['O']] -= penalty\n",
    "\n",
    "    return refined_logits\n",
    "\n",
    "def refine_entity_logits_with_relations_(\n",
    "    entity_logits: torch.Tensor,\n",
    "    edge_probs: torch.Tensor,\n",
    "    edge_indices: torch.Tensor,\n",
    "    o_label_id: int,\n",
    "    alpha: float = 0.5,\n",
    "    rel_threshold: float = 0.5,\n",
    "):\n",
    "    refined_logits = entity_logits.clone()\n",
    "    device = entity_logits.device\n",
    "    edge_indices = edge_indices.to(device)\n",
    "    edge_probs = edge_probs.to(device)\n",
    "\n",
    "    # Identify confident edges\n",
    "    confident_edges = (edge_probs > rel_threshold).any(dim=1)\n",
    "    if confident_edges.sum() == 0:\n",
    "        return refined_logits\n",
    "\n",
    "    src = edge_indices[0][confident_edges]\n",
    "    tgt = edge_indices[1][confident_edges]\n",
    "\n",
    "    # Nodes involved in confident relations\n",
    "    involved_nodes = torch.unique(torch.cat([src, tgt]))\n",
    "\n",
    "    # Penalize O\n",
    "    refined_logits[involved_nodes, o_label_id] -= alpha\n",
    "\n",
    "    return refined_logits\n",
    "\n",
    "\n",
    "def boost_label1(entity_logits: torch.Tensor, label1_id: int = 1, alpha: float = 0.5):\n",
    "    \"\"\"\n",
    "    Simply increases the logit of label1 for all nodes.\n",
    "    \"\"\"\n",
    "    refined_logits = entity_logits.clone()\n",
    "    refined_logits[:, label1_id] += alpha\n",
    "    return refined_logits\n",
    "\n",
    "\n",
    "def refine_entity_logits_with_relations(\n",
    "    entity_logits,\n",
    "    edge_probs_joint,\n",
    "    num_nodes,\n",
    "    alpha=0.1\n",
    "):\n",
    "    refined_logits = entity_logits.clone()\n",
    "\n",
    "    # Upper-triangular edge indexing (must match your edge construction)\n",
    "    edge_indices = torch.triu_indices(num_nodes, num_nodes, offset=1)\n",
    "    src_idx = edge_indices[0]\n",
    "    tgt_idx = edge_indices[1]\n",
    "\n",
    "    # Use max relation confidence per edge\n",
    "    rel_strength = edge_probs_joint.max(dim=1).values  # [E]\n",
    "    #print(edge_probs_joint.shape, rel_strength.shape)\n",
    "\n",
    "    for e, strength in enumerate(rel_strength):\n",
    "        if strength <= 0:\n",
    "            continue\n",
    "\n",
    "        i = src_idx[e]\n",
    "        j = tgt_idx[e]\n",
    "        refined_logits[i] += alpha #* strength\n",
    "        refined_logits[j] += alpha #* strength\n",
    "\n",
    "    return refined_logits\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def edge_type_statistics(edge_all_y, edge_all_preds, edge_src_gt, edge_tgt_gt, relation_type_mapping):\n",
    "\n",
    "    inverted_relation_mapping = {v: k for k, v in relation_type_mapping.items()}\n",
    "    \n",
    "    # Group edges by (src_entity_type, tgt_entity_type, relation_type)\n",
    "    metrics_by_group = defaultdict(list)\n",
    "    \n",
    "    # Determine the relation type for each edge (argmax for simplicity)\n",
    "    edge_true_type = np.argmax(edge_all_y, axis=1)\n",
    "    \n",
    "    for i, (rel, src, tgt) in enumerate(zip(edge_true_type, edge_src_gt, edge_tgt_gt)):\n",
    "        metrics_by_group[(src, tgt, rel)].append(i)\n",
    "    \n",
    "    # Compute F1, precision, recall per group\n",
    "    stats = {}\n",
    "    for (src, tgt, rel), indices in metrics_by_group.items():\n",
    "        y_true = edge_all_y[indices]\n",
    "        y_pred = edge_all_preds[indices]\n",
    "\n",
    "        f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "        precision = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "\n",
    "        stats[(src, tgt, inverted_relation_mapping[rel])] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'support': y_true.sum(),\n",
    "            'predicted': y_pred.sum()\n",
    "        }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def summarize_edge_errors(stats, inverted_relation_mapping, id2iob):\n",
    "    \"\"\"\n",
    "    Summarize which source-target label types produce errors.\n",
    "    stats: dict with keys (relation_id, src_node, tgt_node)\n",
    "           and values {'tp', 'fp', 'fn', 'support', 'predicted'}\n",
    "    \"\"\"\n",
    "    error_summary = defaultdict(lambda: {\"fp\":0, \"fn\":0, \"support\":0, \"predicted\":0})\n",
    "\n",
    "    for (r, src, tgt), v in stats.items():\n",
    "        rel_name = inverted_relation_mapping[r]\n",
    "        src_prefix = id2iob[src].split('-')[0] if '-' in id2iob[src] else id2iob[src]\n",
    "        tgt_prefix = id2iob[tgt].split('-')[0] if '-' in id2iob[tgt] else id2iob[tgt]\n",
    "\n",
    "        key = (rel_name, src_prefix, tgt_prefix)\n",
    "\n",
    "        error_summary[key][\"fp\"] += v[\"fp\"]\n",
    "        error_summary[key][\"fn\"] += v[\"fn\"]\n",
    "        error_summary[key][\"support\"] += v[\"support\"]\n",
    "        error_summary[key][\"predicted\"] += v[\"predicted\"]\n",
    "\n",
    "    # Print sorted by relation\n",
    "    for rel in sorted(set(k[0] for k in error_summary.keys())):\n",
    "        print(f\"\\n=== {rel} ===\")\n",
    "        for (r, src_p, tgt_p), v in sorted(error_summary.items()):\n",
    "            if r != rel:\n",
    "                continue\n",
    "            print(f\"{src_p} → {tgt_p} | FP={v['fp']} FN={v['fn']} Support={v['support']} Predicted={v['predicted']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report\n",
    "from sklearn.metrics import f1_score as sklearn_f1, precision_score as sklearn_p, recall_score as sklearn_r\n",
    "from collections import defaultdict\n",
    "\n",
    "def edge_index_2_adj_matrix_label(data):\n",
    "    #print(data)\n",
    "    matrix = torch.squeeze(to_dense_adj(data.target_edge_index, edge_attr=data.y_edge_attr, max_num_nodes=data.x.shape[0]))\n",
    "    triu_indices = torch.triu_indices(matrix.shape[0], matrix.shape[0], offset=1)\n",
    "    triu_mask = torch.squeeze(to_dense_adj(triu_indices)).bool()\n",
    "    #print(edge_logits.squeeze().shape, batch_targets[triu_mask].shape)\n",
    "    \n",
    "    return matrix[triu_mask]\n",
    "\n",
    "def evaluate_bert_4(model, eval_dataloader, device, pipeline=False,\n",
    "                    node_type_mapping=None, relation_type_mapping=None):\n",
    "    \n",
    "    node_type_mapping = id2iob\n",
    "    if relation_type_mapping is None:\n",
    "        raise ValueError(\"relation_type_mapping must be provided\")\n",
    "    inverted_relation_mapping = {v: k for k, v in relation_type_mapping.items()}\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    edge_all_y = []\n",
    "    edge_all_preds_joint = []\n",
    "    edge_src_types = []\n",
    "    edge_tgt_types = []\n",
    "    print(relation_type_mapping)\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_dataloader):\n",
    "            # Forward pass\n",
    "            entity_logits, edge_logits_joint = model(batch, pipeline=pipeline)\n",
    "\n",
    "        \n",
    "\n",
    "            # Edge processing\n",
    "            edge_y = edge_index_2_adj_matrix_label(batch)  # [num_edges, num_relation_types]\n",
    "            edge_probs_joint = torch.sigmoid(edge_logits_joint)\n",
    "            edge_preds_joint = (edge_probs_joint > 0.5).int()\n",
    "\n",
    "            edge_all_y.extend(edge_y.cpu())\n",
    "            edge_all_preds_joint.extend(edge_preds_joint.cpu())\n",
    "            \n",
    "            # Determine node types\n",
    "            num_nodes = batch.x.shape[0]\n",
    "            edge_indices = torch.triu_indices(num_nodes, num_nodes, offset=1)\n",
    "            src_idx = edge_indices[0]\n",
    "            tgt_idx = edge_indices[1]\n",
    "            \n",
    "                        # Entity predictions\n",
    "                        \n",
    "            #entity_logits_refined = refine_entity_logits_with_relations_(\n",
    "            #    entity_logits,\n",
    "            #    edge_probs_joint,\n",
    "            #    edge_indices,\n",
    "            #    o_label_id=iob2id['O'],\n",
    "            #    alpha=1\n",
    "            #)\n",
    "            #entity_logits_refined = boost_label1(entity_logits, label1_id=1, alpha=10)\n",
    "\n",
    "            predictions = torch.argmax(entity_logits, dim=-1)\n",
    "            entity_labels = batch.labels  # ground-truth node labels\n",
    "\n",
    "\n",
    "\n",
    "            # Use predicted or ground-truth entity types as node types\n",
    "            if pipeline:\n",
    "                node_type_ids = predictions#batch.labels\n",
    "            else:\n",
    "                node_type_ids = predictions#batch.labels\n",
    "            #print(src_idx.shape, tgt_idx.shape, node_type_ids.shape)\n",
    "            src_types = node_type_ids[src_idx].cpu().numpy()\n",
    "            tgt_types = node_type_ids[tgt_idx].cpu().numpy()\n",
    "            edge_src_types.extend(src_types)\n",
    "            edge_tgt_types.extend(tgt_types)\n",
    "\n",
    "            # Convert entity labels to IOB format\n",
    "            entity_labels_iob = [id2iob[label.item()] for label in entity_labels]\n",
    "            predictions_iob = [id2iob[pred.item()] for pred in predictions]\n",
    "            all_preds.append(predictions_iob)\n",
    "            all_labels.append(entity_labels_iob)\n",
    "\n",
    "    # Convert edge lists to numpy\n",
    "    edge_all_y = np.array(edge_all_y)\n",
    "    edge_all_preds_joint = np.array(edge_all_preds_joint)\n",
    "    edge_src_types = np.array(edge_src_types)\n",
    "    edge_tgt_types = np.array(edge_tgt_types)\n",
    "    print(edge_all_y.shape, edge_src_types.shape, edge_tgt_types.shape)\n",
    "\n",
    "    # ---- Overall edge metrics ----\n",
    "    #edge_all_preds_joint = mask_predicted_edges(edge_all_preds_joint, edge_src_types, edge_tgt_types, ignore_I=False, ignore_O=True)\n",
    "\n",
    "    edge_joint_f1 = sklearn_f1(edge_all_y, edge_all_preds_joint, average=\"micro\", zero_division=0)\n",
    "    print(f\"\\nOverall Edge Joint F1: {edge_joint_f1:.4f}\")\n",
    "\n",
    "\n",
    "    # ---- Edge metrics per relation type and node-type pair with counts ----\n",
    "    x = {}\n",
    "    y = {}\n",
    "    \n",
    "    print(\"\\n=== Edge metrics per (relation, src_type, tgt_type) ===\")\n",
    "\n",
    "    stats = defaultdict(lambda: {\n",
    "        \"tp\": 0,\n",
    "        \"fp\": 0,\n",
    "        \"fn\": 0,\n",
    "        \"support\": 0,\n",
    "        \"predicted\": 0\n",
    "    })\n",
    "\n",
    "    num_edges = edge_all_y.shape[0]\n",
    "    num_relations = edge_all_y.shape[1]\n",
    "\n",
    "\n",
    "    m = {} # count edges between I or O\n",
    "    for i in range(num_edges):\n",
    "        src = edge_src_types[i]\n",
    "        tgt = edge_tgt_types[i]\n",
    "        src, tgt = tuple(sorted((src, tgt)))\n",
    "        for r in range(num_relations):\n",
    "            key = (r, src, tgt)\n",
    "\n",
    "            y_t = edge_all_y[i, r]\n",
    "            y_p = edge_all_preds_joint[i, r]\n",
    "\n",
    "            if y_t == 1:\n",
    "                stats[key][\"support\"] += 1\n",
    "            if y_p == 1:\n",
    "                stats[key][\"predicted\"] += 1\n",
    "\n",
    "            \n",
    "            if y_t == 1 and y_p == 1:\n",
    "                stats[key][\"tp\"] += 1\n",
    "            elif y_t == 0 and y_p == 1:\n",
    "                stats[key][\"fp\"] += 1\n",
    "            elif y_t == 1 and y_p == 0:\n",
    "                stats[key][\"fn\"] += 1\n",
    "                \n",
    "                \n",
    "    sorted_stats = sorted(\n",
    "        stats.items(),\n",
    "        key=lambda item: inverted_relation_mapping[item[0][0]]  # sort by rel_name\n",
    "    )\n",
    "                \n",
    "    for (r, src, tgt), v in sorted_stats:\n",
    "        tp, fp, fn = v[\"tp\"], v[\"fp\"], v[\"fn\"]\n",
    "\n",
    "        if v[\"support\"] == 0 and v[\"predicted\"] == 0:\n",
    "            continue\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = (\n",
    "            2 * precision * recall / (precision + recall)\n",
    "            if (precision + recall) > 0 else 0.0\n",
    "        )\n",
    "\n",
    "        rel_name = inverted_relation_mapping[r]\n",
    "        src_name = node_type_mapping[src]\n",
    "        tgt_name = node_type_mapping[tgt]\n",
    "\n",
    "        print(\n",
    "            f\"{rel_name:15s}: \"\n",
    "            f\"{src_name} → {tgt_name} | \"\n",
    "            f\"P={precision:.4f} R={recall:.4f} F1={f1:.4f} | \"\n",
    "            f\"Support={v['support']} Predicted={v['predicted']}\"\n",
    "        ) \n",
    "        x[rel_name] = x.get(rel_name, 0) + v['predicted']\n",
    "        y[rel_name] = y.get(rel_name, 0) + v['support']\n",
    "        if \"I-\" in src_name or \"I-\" in tgt_name or src_name == \"O\" or tgt_name == \"O\":\n",
    "            m[\"I\"] = m.get(\"I\", 0) + v['predicted']     \n",
    "    #---- Summarize edge errors ----\n",
    "    print(\"\\n=== Edge error summary ===\")\n",
    "    summarize_edge_errors(stats, inverted_relation_mapping, node_type_mapping)\n",
    "    # ---- Entity metrics ----\n",
    "    f1_entity = metric.compute(predictions=all_preds, references=all_labels)[\"overall_f1\"]\n",
    "    print(\"\\nEntity metrics by label:\")\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n",
    "    print(f\"Entity F1 score: {f1_entity:.4f}\")\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(m)\n",
    "    return f1_entity, edge_joint_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TLINK': 0, 'SRLINK': 1, 'QSLINK': 2, 'OLINK': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:03<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(342474, 4) (342474,) (342474,)\n",
      "\n",
      "Overall Edge Joint F1: 0.5576\n",
      "\n",
      "=== Edge metrics per (relation, src_type, tgt_type) ===\n",
      "OLINK          : O → O | P=0.0000 R=0.0000 F1=0.0000 | Support=3 Predicted=0\n",
      "OLINK          : O → B-Participant | P=0.4444 R=0.0952 F1=0.1569 | Support=42 Predicted=9\n",
      "OLINK          : B-Participant → B-Participant | P=0.4927 R=0.5488 F1=0.5192 | Support=246 Predicted=274\n",
      "OLINK          : B-Participant → B-Spatial_Relation | P=0.4444 R=0.5714 F1=0.5000 | Support=14 Predicted=18\n",
      "OLINK          : B-Participant → I-Participant | P=0.5455 R=0.1500 F1=0.2353 | Support=40 Predicted=11\n",
      "OLINK          : I-Participant → I-Participant | P=0.0000 R=0.0000 F1=0.0000 | Support=10 Predicted=0\n",
      "OLINK          : I-Participant → B-Spatial_Relation | P=0.0000 R=0.0000 F1=0.0000 | Support=2 Predicted=0\n",
      "OLINK          : I-Event → B-Participant | P=0.0000 R=0.0000 F1=0.0000 | Support=1 Predicted=0\n",
      "OLINK          : B-Participant → I-Spatial_Relation | P=0.3333 R=0.5000 F1=0.4000 | Support=2 Predicted=3\n",
      "OLINK          : B-Event → B-Participant | P=0.0000 R=0.0000 F1=0.0000 | Support=1 Predicted=1\n",
      "OLINK          : O → I-Participant | P=0.0000 R=0.0000 F1=0.0000 | Support=3 Predicted=0\n",
      "OLINK          : B-Spatial_Relation → B-Spatial_Relation | P=0.6667 R=0.5714 F1=0.6154 | Support=7 Predicted=6\n",
      "OLINK          : O → B-Spatial_Relation | P=0.0000 R=0.0000 F1=0.0000 | Support=1 Predicted=0\n",
      "OLINK          : B-Event → B-Spatial_Relation | P=0.0000 R=0.0000 F1=0.0000 | Support=0 Predicted=1\n",
      "QSLINK         : O → B-Event | P=0.0000 R=0.0000 F1=0.0000 | Support=5 Predicted=1\n",
      "QSLINK         : O → B-Participant | P=0.2500 R=0.0526 F1=0.0870 | Support=19 Predicted=4\n",
      "QSLINK         : B-Participant → I-Participant | P=0.0000 R=0.0000 F1=0.0000 | Support=10 Predicted=5\n",
      "QSLINK         : I-Participant → I-Participant | P=0.0000 R=0.0000 F1=0.0000 | Support=4 Predicted=3\n",
      "QSLINK         : B-Participant → B-Spatial_Relation | P=0.2754 R=0.7600 F1=0.4043 | Support=25 Predicted=69\n",
      "QSLINK         : I-Participant → B-Spatial_Relation | P=0.2000 R=0.1111 F1=0.1429 | Support=9 Predicted=5\n",
      "QSLINK         : B-Event → B-Participant | P=0.0000 R=0.0000 F1=0.0000 | Support=5 Predicted=2\n",
      "QSLINK         : B-Event → B-Spatial_Relation | P=0.6667 R=0.3333 F1=0.4444 | Support=12 Predicted=6\n",
      "QSLINK         : B-Spatial_Relation → I-Spatial_Relation | P=0.0000 R=0.0000 F1=0.0000 | Support=0 Predicted=3\n",
      "QSLINK         : B-Participant → B-Participant | P=0.0000 R=0.0000 F1=0.0000 | Support=4 Predicted=0\n",
      "QSLINK         : I-Event → B-Spatial_Relation | P=0.0000 R=0.0000 F1=0.0000 | Support=1 Predicted=0\n",
      "QSLINK         : B-Spatial_Relation → B-Spatial_Relation | P=0.3750 R=0.7500 F1=0.5000 | Support=4 Predicted=8\n",
      "QSLINK         : O → O | P=1.0000 R=1.0000 F1=1.0000 | Support=1 Predicted=1\n",
      "QSLINK         : O → B-Spatial_Relation | P=0.0000 R=0.0000 F1=0.0000 | Support=1 Predicted=0\n",
      "SRLINK         : O → O | P=0.0000 R=0.0000 F1=0.0000 | Support=23 Predicted=0\n",
      "SRLINK         : B-Event → B-Participant | P=0.6482 R=0.8049 F1=0.7181 | Support=451 Predicted=560\n",
      "SRLINK         : B-Event → B-Event | P=0.6222 R=0.7925 F1=0.6971 | Support=212 Predicted=270\n",
      "SRLINK         : O → B-Event | P=0.4545 R=0.1163 F1=0.1852 | Support=43 Predicted=11\n",
      "SRLINK         : B-Event → B-Spatial_Relation | P=0.6364 R=0.4828 F1=0.5490 | Support=29 Predicted=22\n",
      "SRLINK         : O → B-Participant | P=0.0000 R=0.0000 F1=0.0000 | Support=23 Predicted=1\n",
      "SRLINK         : I-Event → B-Participant | P=0.7500 R=0.3000 F1=0.4286 | Support=10 Predicted=4\n",
      "SRLINK         : O → I-Participant | P=0.0000 R=0.0000 F1=0.0000 | Support=2 Predicted=0\n",
      "SRLINK         : B-Event → I-Event | P=0.5000 R=0.3846 F1=0.4348 | Support=13 Predicted=10\n",
      "SRLINK         : B-Event → I-Participant | P=0.5333 R=0.2581 F1=0.3478 | Support=31 Predicted=15\n",
      "SRLINK         : B-Participant → B-Participant | P=1.0000 R=0.3333 F1=0.5000 | Support=3 Predicted=1\n",
      "SRLINK         : B-Event → I-Spatial_Relation | P=0.0000 R=0.0000 F1=0.0000 | Support=0 Predicted=3\n",
      "SRLINK         : I-Event → I-Event | P=0.0000 R=0.0000 F1=0.0000 | Support=1 Predicted=0\n",
      "SRLINK         : B-Participant → I-Participant | P=0.5000 R=0.2500 F1=0.3333 | Support=4 Predicted=2\n",
      "SRLINK         : I-Participant → I-Participant | P=0.0000 R=0.0000 F1=0.0000 | Support=2 Predicted=0\n",
      "SRLINK         : B-Event → B-Time | P=0.0000 R=0.0000 F1=0.0000 | Support=0 Predicted=1\n",
      "SRLINK         : I-Event → I-Participant | P=0.0000 R=0.0000 F1=0.0000 | Support=1 Predicted=0\n",
      "TLINK          : B-Time → B-Time | P=0.5000 R=0.8462 F1=0.6286 | Support=13 Predicted=22\n",
      "TLINK          : O → O | P=0.0000 R=0.0000 F1=0.0000 | Support=7 Predicted=0\n",
      "TLINK          : O → B-Event | P=0.6667 R=0.0351 F1=0.0667 | Support=57 Predicted=3\n",
      "TLINK          : B-Event → B-Time | P=0.6842 R=0.8667 F1=0.7647 | Support=30 Predicted=38\n",
      "TLINK          : B-Event → B-Event | P=0.5064 R=0.6420 F1=0.5662 | Support=433 Predicted=549\n",
      "TLINK          : B-Event → I-Event | P=0.2857 R=0.0833 F1=0.1290 | Support=24 Predicted=7\n",
      "TLINK          : B-Event → B-Participant | P=0.4000 R=0.2500 F1=0.3077 | Support=8 Predicted=5\n",
      "TLINK          : B-Participant → B-Time | P=0.0000 R=0.0000 F1=0.0000 | Support=1 Predicted=1\n",
      "TLINK          : O → B-Time | P=0.0000 R=0.0000 F1=0.0000 | Support=2 Predicted=0\n",
      "TLINK          : I-Event → I-Event | P=0.0000 R=0.0000 F1=0.0000 | Support=1 Predicted=0\n",
      "TLINK          : B-Event → I-Participant | P=0.3750 R=0.4286 F1=0.4000 | Support=7 Predicted=8\n",
      "TLINK          : B-Event → I-Time | P=0.0000 R=0.0000 F1=0.0000 | Support=3 Predicted=0\n",
      "TLINK          : B-Time → I-Time | P=1.0000 R=0.3333 F1=0.5000 | Support=3 Predicted=1\n",
      "TLINK          : I-Time → I-Time | P=0.0000 R=0.0000 F1=0.0000 | Support=1 Predicted=0\n",
      "\n",
      "=== Edge error summary ===\n",
      "\n",
      "=== OLINK ===\n",
      "B → B | FP=153 FN=121 Support=268 Predicted=300\n",
      "B → I | FP=7 FN=35 Support=42 Predicted=14\n",
      "I → B | FP=0 FN=3 Support=3 Predicted=0\n",
      "I → I | FP=0 FN=10 Support=10 Predicted=0\n",
      "O → B | FP=5 FN=39 Support=43 Predicted=9\n",
      "O → I | FP=0 FN=3 Support=3 Predicted=0\n",
      "O → O | FP=0 FN=3 Support=3 Predicted=0\n",
      "\n",
      "=== QSLINK ===\n",
      "B → B | FP=59 FN=24 Support=50 Predicted=85\n",
      "B → I | FP=8 FN=10 Support=10 Predicted=8\n",
      "I → B | FP=4 FN=9 Support=10 Predicted=5\n",
      "I → I | FP=3 FN=4 Support=4 Predicted=3\n",
      "O → B | FP=4 FN=24 Support=25 Predicted=5\n",
      "O → O | FP=0 FN=0 Support=1 Predicted=1\n",
      "\n",
      "=== SRLINK ===\n",
      "B → B | FP=308 FN=149 Support=695 Predicted=854\n",
      "B → I | FP=16 FN=34 Support=48 Predicted=30\n",
      "I → B | FP=1 FN=7 Support=10 Predicted=4\n",
      "I → I | FP=0 FN=4 Support=4 Predicted=0\n",
      "O → B | FP=7 FN=61 Support=66 Predicted=12\n",
      "O → I | FP=0 FN=2 Support=2 Predicted=0\n",
      "O → O | FP=0 FN=23 Support=23 Predicted=0\n",
      "\n",
      "=== TLINK ===\n",
      "B → B | FP=298 FN=168 Support=485 Predicted=615\n",
      "B → I | FP=10 FN=31 Support=37 Predicted=16\n",
      "I → I | FP=0 FN=2 Support=2 Predicted=0\n",
      "O → B | FP=1 FN=57 Support=59 Predicted=3\n",
      "O → O | FP=0 FN=7 Support=7 Predicted=0\n",
      "\n",
      "Entity metrics by label:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           Event     0.8422    0.8506    0.8464       502\n",
      "     Participant     0.7152    0.7452    0.7299       573\n",
      "Spatial_Relation     0.4366    0.4627    0.4493        67\n",
      "            Time     0.7206    0.7778    0.7481        63\n",
      "\n",
      "       micro avg     0.7514    0.7751    0.7631      1205\n",
      "       macro avg     0.6787    0.7091    0.6934      1205\n",
      "    weighted avg     0.7529    0.7751    0.7638      1205\n",
      "\n",
      "Entity F1 score: 0.7631\n",
      "{'OLINK': 323, 'QSLINK': 107, 'SRLINK': 900, 'TLINK': 634}\n",
      "{'OLINK': 372, 'QSLINK': 100, 'SRLINK': 848, 'TLINK': 590}\n",
      "{'I': 110}\n",
      "Final Test Edge F1: 0.5576, Entity F1: 0.7631\n"
     ]
    }
   ],
   "source": [
    "test_entity_f1, test_edge_f1 = evaluate_bert_4(model, test_loader, device, relation_type_mapping=relation_type_mapping)\n",
    "print(f\"Final Test Edge F1: {test_edge_f1:.4f}, Entity F1: {test_entity_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
